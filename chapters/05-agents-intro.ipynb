{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kkzhY6yaJvra"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/aurelio-labs/langchain-course/blob/main/chapters/05-agents-intro.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tjdh8R2hHyBe"
      },
      "source": [
        "#### LangChain Essentials Course"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pGyuCmfjHyBf"
      },
      "source": [
        "# LangChain Agents Intro"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v6pVPE5wHyBf"
      },
      "source": [
        "LangChain is one of the most popular open source libraries for AI Engineers. It's goal is to abstract away the complexity in building AI software, provide easy-to-use building blocks, and make it easier when switching between AI service providers.\n",
        "\n",
        "In this example, we will introduce LangChain's Agents, adding the ability to use tools such as search and calculators to complete tasks that normal LLMs cannot fufil. In this example we will be using OpenAI's `gpt-4o-mini`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "7fUwugBtH00s"
      },
      "outputs": [],
      "source": [
        "!pip install -qU \\\n",
        "  langchain-core==0.3.33 \\\n",
        "  langchain-openai==0.3 \\\n",
        "  langchain-groq==0.2.0 \\\n",
        "  langchain-community==0.3.16 \\\n",
        "  langsmith==0.3.4 \\\n",
        "  google-search-results==2.4.2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xj6kiBZ-HyBg"
      },
      "source": [
        "---\n",
        "\n",
        "> ⚠️ We will be using OpenAI for this example allowing us to run everything via API. If you would like to use Ollama instead, check out the [Ollama LangChain Course](https://github.com/aurelio-labs/langchain-course/tree/main/notebooks/ollama).\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lz64zYRKHyBg"
      },
      "source": [
        "---\n",
        "\n",
        "> ⚠️ If using LangSmith, add your API key below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x1E7bzVbHyBg",
        "outputId": "7a0e7377-4172-4882-e184-8338b0ada776"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter LangSmith API Key: ··········\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from getpass import getpass\n",
        "\n",
        "os.environ[\"LANGCHAIN_API_KEY\"] = os.getenv(\"LANGCHAIN_API_KEY\") or \\\n",
        "    getpass(\"Enter LangSmith API Key: \")\n",
        "\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
        "os.environ[\"LANGCHAIN_PROJECT\"] = \"default\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xVozCSrXHyBg"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WgbH7TfgHyBg"
      },
      "source": [
        "## Introduction to Tools"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NZp6eo8RHyBg"
      },
      "source": [
        "Tools are a way augment our LLMs with code execution. A tool is simply a function formatted so that our agent can undertstand how to use it, and then execute it. Let's start by creating a few simple tools.\n",
        "\n",
        "We can use the `@tool` decorator to create an LLM-compatible tool from a standard python function — this function should include a few things for optimal performance:\n",
        "\n",
        "* A docstring describing what the tool does and when it should be used, this will be read by our LLM/agent and used to decide when to use the tool, and also how to use the tool.\n",
        "\n",
        "* Clear parameter names that ideally tell the LLM what each parameter is, if it isn't clear we make sure the docstring explains what the parameter is for and how to use it.\n",
        "\n",
        "* Both parameter and return type annotations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Raw-I6J9HyBh"
      },
      "outputs": [],
      "source": [
        "from langchain_core.tools import tool\n",
        "\n",
        "@tool\n",
        "def add(x: float, y: float) -> float:\n",
        "    \"\"\"Add 'x' and 'y'.\"\"\"\n",
        "    return x + y\n",
        "\n",
        "@tool\n",
        "def multiply(x: float, y: float) -> float:\n",
        "    \"\"\"Multiply 'x' and 'y'.\"\"\"\n",
        "    return x * y\n",
        "\n",
        "@tool\n",
        "def exponentiate(x: float, y: float) -> float:\n",
        "    \"\"\"Raise 'x' to the power of 'y'.\"\"\"\n",
        "    return x ** y\n",
        "\n",
        "@tool\n",
        "def subtract(x: float, y: float) -> float:\n",
        "    \"\"\"Subtract 'x' from 'y'.\"\"\"\n",
        "    return y - x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6pts3fTiHyBh"
      },
      "source": [
        "With the `@tool` decorator our function is turned into a `StructuredTool` object, which we can see below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kb5tBy0NHyBh",
        "outputId": "4d5a9184-c7cb-4a7b-f5e9-cc5517ed94b8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "StructuredTool(name='add', description=\"Add 'x' and 'y'.\", args_schema=<class 'langchain_core.utils.pydantic.add'>, func=<function add at 0x7ff040e3bec0>)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "add"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yEBnnesZHyBh"
      },
      "source": [
        "We can see the tool name, description, and arg schema:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K5jYtOtqHyBh",
        "outputId": "1a13f66e-312a-4aa2-83f5-e29e531f4291"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "add.name='add'\n",
            "add.description=\"Add 'x' and 'y'.\"\n"
          ]
        }
      ],
      "source": [
        "print(f\"{add.name=}\\n{add.description=}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OQS-TOjjHyBh",
        "outputId": "88b5a072-53b1-43f6-cd0a-7671468c0132"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'description': \"Add 'x' and 'y'.\",\n",
              " 'properties': {'x': {'title': 'X', 'type': 'number'},\n",
              "  'y': {'title': 'Y', 'type': 'number'}},\n",
              " 'required': ['x', 'y'],\n",
              " 'title': 'add',\n",
              " 'type': 'object'}"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "add.args_schema.model_json_schema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XMqevcXOHyBi",
        "outputId": "703ac6a9-d45d-4017-db6e-59f6774d6001"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'description': \"Raise 'x' to the power of 'y'.\",\n",
              " 'properties': {'x': {'title': 'X', 'type': 'number'},\n",
              "  'y': {'title': 'Y', 'type': 'number'}},\n",
              " 'required': ['x', 'y'],\n",
              " 'title': 'exponentiate',\n",
              " 'type': 'object'}"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "exponentiate.args_schema.model_json_schema()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7rI3zmLLHyBi"
      },
      "source": [
        "When invoking the tool, a JSON string output by the LLM will be parsed into JSON and then consumed as kwargs, similar to the below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I4xjfsVEHyBi",
        "outputId": "0afdb59b-e7cc-4678-bb4b-2a7e277748d4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'x': 5, 'y': 2}"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "import json\n",
        "\n",
        "llm_output_string = \"{\\\"x\\\": 5, \\\"y\\\": 2}\"  # this is the output from the LLM\n",
        "llm_output_dict = json.loads(llm_output_string)  # load as dictionary\n",
        "llm_output_dict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wmgV2kgqHyBi"
      },
      "source": [
        "This is then passed into the tool function as `kwargs` (keyword arguments) as indicated by the `**` operator - the `**` operator is used to unpack the dictionary into keyword arguments."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rXMZrD7jHyBi",
        "outputId": "97b36187-c951-4d03-f7e5-8da18800b5c7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "25"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "exponentiate.func(**llm_output_dict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aeHgjrBAHyBi"
      },
      "source": [
        "This covers the basics of tools and how they work, let's move on to creating the agent itself."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uOLTTES4HyBi"
      },
      "source": [
        "## Creating an Agent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-PVl8e8rHyBi"
      },
      "source": [
        "We're going to construct a simple tool calling agent. We will use **L**ang**C**hain **E**pression **L**anguage (LCEL) to construct the agent. We will cover LCEL more in the next chapter, but for now - all we need to know is that our agent will be constructed using syntax and components like so:\n",
        "\n",
        "\n",
        "```\n",
        "agent = (\n",
        "    <input parameters, including chat history and user query>\n",
        "    | <prompt>\n",
        "    | <LLM with tools>\n",
        ")\n",
        "```\n",
        "\n",
        "We need this agent to remember previous interactions within the conversation. To do that, we will use the `ChatPromptTemplate` with a system message, a placeholder for our chat history, a placeholder for the user query, and finally a placeholder for the agent scratchpad.\n",
        "\n",
        "The agent scratchpad is where the agent will write it's _\"notes\"_ as it is working through multiple internal thought and tool-use steps to produce a final output to the user."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "d3bE5bipHyBi"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"you're a helpful assistant\"),\n",
        "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
        "    (\"human\", \"{input}\"),\n",
        "    (\"placeholder\", \"{agent_scratchpad}\"),\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "buOVKBS2HyBi"
      },
      "source": [
        "Next, we must define our LLM, we will use the `gpt-4o-mini` model with a `temperature` of `0.0`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2JA_sSKeHyBi",
        "outputId": "0992e087-651b-4cf5-f8a6-5f33f7e5ef8a",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain-groq==0.2.0 in /usr/local/lib/python3.11/dist-packages (0.2.0)\n",
            "Requirement already satisfied: langchain-core==0.3.0 in /usr/local/lib/python3.11/dist-packages (0.3.0)\n",
            "Requirement already satisfied: pydantic==2.7.0 in /usr/local/lib/python3.11/dist-packages (2.7.0)\n",
            "Requirement already satisfied: groq<1,>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from langchain-groq==0.2.0) (0.25.0)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-core==0.3.0) (6.0.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core==0.3.0) (1.33)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.117 in /usr/local/lib/python3.11/dist-packages (from langchain-core==0.3.0) (0.1.147)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core==0.3.0) (24.2)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core==0.3.0) (8.5.0)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core==0.3.0) (4.13.2)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic==2.7.0) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.1 in /usr/local/lib/python3.11/dist-packages (from pydantic==2.7.0) (2.18.1)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from groq<1,>=0.4.1->langchain-groq==0.2.0) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from groq<1,>=0.4.1->langchain-groq==0.2.0) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from groq<1,>=0.4.1->langchain-groq==0.2.0) (0.28.1)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from groq<1,>=0.4.1->langchain-groq==0.2.0) (1.3.1)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core==0.3.0) (3.0.0)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.2.0,>=0.1.117->langchain-core==0.3.0) (3.10.18)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.2.0,>=0.1.117->langchain-core==0.3.0) (2.32.3)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.2.0,>=0.1.117->langchain-core==0.3.0) (1.0.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->groq<1,>=0.4.1->langchain-groq==0.2.0) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->groq<1,>=0.4.1->langchain-groq==0.2.0) (2025.4.26)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->groq<1,>=0.4.1->langchain-groq==0.2.0) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq<1,>=0.4.1->langchain-groq==0.2.0) (0.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.117->langchain-core==0.3.0) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.117->langchain-core==0.3.0) (2.4.0)\n"
          ]
        }
      ],
      "source": [
        "# Install required packages\n",
        "!pip install langchain-groq==0.2.0 langchain-core==0.3.0 pydantic==2.7.0\n",
        "\n",
        "# Import modules\n",
        "from langchain_groq import ChatGroq\n",
        "from langchain_core.caches import BaseCache\n",
        "import os\n",
        "\n",
        "# Rebuild the model to resolve Pydantic issues\n",
        "ChatGroq.model_rebuild()\n",
        "\n",
        "# Set your Groq API key (replace with your actual key)\n",
        "os.environ[\"GROQ_API_KEY\"] = \"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\"\n",
        "\n",
        "# Instantiate ChatGroq\n",
        "llm = ChatGroq(\n",
        "    model=\"llama-3.1-8b-instant\",\n",
        "    temperature=0,\n",
        "    max_tokens=None,\n",
        "    timeout=None,\n",
        "    max_retries=2\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d7_xvc8eHyBi"
      },
      "source": [
        "When creating an agent we need to add conversational memory to make the agent remember previous interactions. We'll be using the older `ConversationBufferMemory` class rather than the newer `RunnableWithMessageHistory` — the reason being that we will also be using the older `create_tool_calling_agent` and `AgentExecutor` method and class.\n",
        "\n",
        "In the `05` chapter we will be using the newer `RunnableWithMessageHistory` class as we'll be building a custom `AgentExecutor`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xvhle9X2HyBi",
        "outputId": "ea3fde16-250d-4686-8034-67a30d5934ba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-07edbbd232c1>:3: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
            "  memory = ConversationBufferMemory(\n"
          ]
        }
      ],
      "source": [
        "from langchain.memory import ConversationBufferMemory\n",
        "\n",
        "memory = ConversationBufferMemory(\n",
        "    memory_key=\"chat_history\",  # must align with MessagesPlaceholder variable_name\n",
        "    return_messages=True  # to return Message objects\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6TkhrZ5OHyBi"
      },
      "source": [
        "Now we will initialize our agent. For that we need:\n",
        "\n",
        "* `llm`: as already defined\n",
        "* `tools`: to be defined (just a list of our previously defined tools)\n",
        "* `prompt`: as already defined\n",
        "* `memory`: as already defined"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "WOrJMHCiHyBj"
      },
      "outputs": [],
      "source": [
        "from langchain.agents import create_tool_calling_agent\n",
        "\n",
        "tools = [add, subtract, multiply, exponentiate]\n",
        "\n",
        "agent = create_tool_calling_agent(\n",
        "    llm=llm, tools=tools, prompt=prompt\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MOaGuo4PHyBj"
      },
      "source": [
        "Our `agent` by itself is like one-step of our agent execution loop. So, if we call the `agent.invoke` method it will get the LLM to generate a single response and go no further, so no tools will be executed, and no next iterations will be performed.\n",
        "\n",
        "We can see this by asking a query that should trigger a tool call:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eqkrelxIHyBj",
        "outputId": "e375717a-3cdc-4985-a7f6-1239f27fe9eb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AgentFinish(return_values={'output': '<multiply>{\"x\": 10.7, \"y\": 7.68}</multiply>'}, log='<multiply>{\"x\": 10.7, \"y\": 7.68}</multiply>')"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "agent.invoke({\n",
        "    \"input\": \"what is 10.7 multiplied by 7.68?\",\n",
        "    \"chat_history\": memory.chat_memory.messages,\n",
        "    \"intermediate_steps\": []  # agent will append it's internal steps here\n",
        "})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OMUKAebFHyBj"
      },
      "source": [
        "Here, we can see the LLM has generated that we should use the `multiply` tool and the tool input should be `{\"x\": 10.7, \"y\": 7.68}`. However, the tool is not executed. For that to happen we need an agent execution loop, which will handle the multiple iterations of generation to tool calling to generation, etc.\n",
        "\n",
        "We use the `AgentExecutor` class to handle the execution loop:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "v1211mPQHyBj"
      },
      "outputs": [],
      "source": [
        "from langchain.agents import AgentExecutor\n",
        "\n",
        "agent_executor = AgentExecutor(\n",
        "    agent=agent,\n",
        "    tools=tools,\n",
        "    memory=memory,\n",
        "    verbose=True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eYIYOo2LHyBj"
      },
      "source": [
        "Now let's try the same query with the executor, note that the `intermediate_steps` parameter that we added before is no longer needed as the executor handles it internally."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AnM6cRCEHyBj",
        "outputId": "4ef5f750-7428-4dd0-878c-a4007794bd96"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_core.callbacks.manager:Error in StdOutCallbackHandler.on_chain_start callback: AttributeError(\"'NoneType' object has no attribute 'get'\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32;1m\u001b[1;3m<multiply>{\"x\": 10.7, \"y\": 7.68}</multiply>\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input': 'what is 10.7 multiplied by 7.68?',\n",
              " 'chat_history': [HumanMessage(content='what is 10.7 multiplied by 7.68?', additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content='<multiply>{\"x\": 10.7, \"y\": 7.68}</multiply>', additional_kwargs={}, response_metadata={})],\n",
              " 'output': '<multiply>{\"x\": 10.7, \"y\": 7.68}</multiply>'}"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "agent_executor.invoke({\n",
        "    \"input\": \"what is 10.7 multiplied by 7.68?\",\n",
        "    \"chat_history\": memory.chat_memory.messages,\n",
        "})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DztfJ_x0HyBj"
      },
      "source": [
        "We can see that the `multiply` tool was invoked, producing the observation of `82.175999...`. After the observation was provided, we can see that the LLM then generated a final response of:\n",
        "\n",
        "```\n",
        "10.7 multiplied by 7.68 is approximately 82.18.\n",
        "```\n",
        "\n",
        "This final response was generated based on the original query and the tool output (ie the _observation_). We can also confirm that this answer is accurate:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_nJeYJSmHyBj",
        "outputId": "5f8a5129-1ecd-473c-c26d-ca0a9ffaaafa"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "82.17599999999999"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "10.7*7.68"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VC9gxtRTHyBj"
      },
      "source": [
        "Let's test our agent with some memory and tool use. First, we tell it our name, then we will perform a few tool calls, then see if the agent can still recall our name.\n",
        "\n",
        "First, give the agent our name:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aNnFrhYwHyBk",
        "outputId": "0899ea8c-f2cf-44b5-e40c-91cdd199968c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_core.callbacks.manager:Error in StdOutCallbackHandler.on_chain_start callback: AttributeError(\"'NoneType' object has no attribute 'get'\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32;1m\u001b[1;3mThat's nice to know, but it doesn't seem to be related to the previous math problem. If you'd like to continue with the math problem, I can help you with that.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input': 'My name is James',\n",
              " 'chat_history': [HumanMessage(content='what is 10.7 multiplied by 7.68?', additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content='<multiply>{\"x\": 10.7, \"y\": 7.68}</multiply>', additional_kwargs={}, response_metadata={}),\n",
              "  HumanMessage(content='My name is James', additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content=\"That's nice to know, but it doesn't seem to be related to the previous math problem. If you'd like to continue with the math problem, I can help you with that.\", additional_kwargs={}, response_metadata={})],\n",
              " 'output': \"That's nice to know, but it doesn't seem to be related to the previous math problem. If you'd like to continue with the math problem, I can help you with that.\"}"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "agent_executor.invoke({\n",
        "    \"input\": \"My name is James\",\n",
        "    \"chat_history\": memory\n",
        "})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c8Tx9CnoHyBk"
      },
      "source": [
        "Now let's try and get the agent to perform multiple tool calls within a single execution loop:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0tQA-L3qHyBk",
        "outputId": "63899a06-2aeb-4723-9023-f1341f1849fc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_core.callbacks.manager:Error in StdOutCallbackHandler.on_chain_start callback: AttributeError(\"'NoneType' object has no attribute 'get'\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32;1m\u001b[1;3m\n",
            "Invoking: `add` with `{'x': 9, 'y': 10}`\n",
            "\n",
            "\n",
            "\u001b[0m\u001b[36;1m\u001b[1;3m19.0\u001b[0m\u001b[32;1m\u001b[1;3m\n",
            "Invoking: `multiply` with `{'x': 4, 'y': 2}`\n",
            "\n",
            "\n",
            "\u001b[0m\u001b[38;5;200m\u001b[1;3m8.0\u001b[0m\u001b[32;1m\u001b[1;3m\n",
            "Invoking: `exponentiate` with `{'x': 8, 'y': 3}`\n",
            "\n",
            "\n",
            "\u001b[0m\u001b[36;1m\u001b[1;3m512.0\u001b[0m\u001b[32;1m\u001b[1;3m\n",
            "Invoking: `subtract` with `{'x': 512, 'y': 19}`\n",
            "\n",
            "\n",
            "\u001b[0m\u001b[33;1m\u001b[1;3m-493.0\u001b[0m\u001b[32;1m\u001b[1;3mThe result is -493.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input': 'What is nine plus 10, minus 4 * 2, to the power of 3',\n",
              " 'chat_history': [HumanMessage(content='what is 10.7 multiplied by 7.68?', additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content='<multiply>{\"x\": 10.7, \"y\": 7.68}</multiply>', additional_kwargs={}, response_metadata={}),\n",
              "  HumanMessage(content='My name is James', additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content=\"That's nice to know, but it doesn't seem to be related to the previous math problem. If you'd like to continue with the math problem, I can help you with that.\", additional_kwargs={}, response_metadata={}),\n",
              "  HumanMessage(content='What is nine plus 10, minus 4 * 2, to the power of 3', additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content='The result is -493.', additional_kwargs={}, response_metadata={})],\n",
              " 'output': 'The result is -493.'}"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "agent_executor.invoke({\n",
        "    \"input\": \"What is nine plus 10, minus 4 * 2, to the power of 3\",\n",
        "    \"chat_history\": memory\n",
        "})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9-v8pfp1HyBk"
      },
      "source": [
        "Let's confirm that the answer is accurate:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pN8RDoySHyBk",
        "outputId": "ec5d10e1-94c7-4200-f1c3-54ddb8d2eb10"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-493"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "9+10-(4*2)**3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "68FsbORDHyBk"
      },
      "source": [
        "Perfect, now let's see if the agent can still recall our name:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IVJwmPxQHyBk",
        "outputId": "39ceea72-0505-41e4-dd2c-8e2fab96d4e9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_core.callbacks.manager:Error in StdOutCallbackHandler.on_chain_start callback: AttributeError(\"'NoneType' object has no attribute 'get'\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32;1m\u001b[1;3mJames\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input': 'What is my name',\n",
              " 'chat_history': [HumanMessage(content='what is 10.7 multiplied by 7.68?', additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content='<multiply>{\"x\": 10.7, \"y\": 7.68}</multiply>', additional_kwargs={}, response_metadata={}),\n",
              "  HumanMessage(content='My name is James', additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content=\"That's nice to know, but it doesn't seem to be related to the previous math problem. If you'd like to continue with the math problem, I can help you with that.\", additional_kwargs={}, response_metadata={}),\n",
              "  HumanMessage(content='What is nine plus 10, minus 4 * 2, to the power of 3', additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content='The result is -493.', additional_kwargs={}, response_metadata={}),\n",
              "  HumanMessage(content='What is my name', additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content='James', additional_kwargs={}, response_metadata={})],\n",
              " 'output': 'James'}"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "agent_executor.invoke({\n",
        "    \"input\": \"What is my name\",\n",
        "    \"chat_history\": memory\n",
        "})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hfuey3P3HyBk"
      },
      "source": [
        "The agent has successfully recalled our name. Let's move on to another agent example."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EAVfkOWMHyBk"
      },
      "source": [
        "## SerpAPI Weather Agent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7bEP_biMHyBk"
      },
      "source": [
        "In this example, we'll be using the same agent and executor setup as before, but we'll be adding the [SerpAPI](https://serpapi.com/users/sign_in) service to allow our agent to search the web for information.\n",
        "\n",
        "To use this tool, you need an API key, with the free plan you can use up to 100 searches per month."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "20iwu0SQHyBk",
        "outputId": "9f6f767c-10cd-446a-cce0-2482a3ee8d35"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your SerpAPI API key: ··········\n"
          ]
        }
      ],
      "source": [
        "os.environ[\"SERPAPI_API_KEY\"] = os.getenv(\"SERPAPI_API_KEY\") \\\n",
        "    or getpass(\"Enter your SerpAPI API key: \")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-wuBcxESHyBk"
      },
      "source": [
        "Here we will load the `serpapi` tool directly from the prebuilt tools that LangChain provides."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "YSfVtDtvHyBk"
      },
      "outputs": [],
      "source": [
        "from langchain.agents import load_tools\n",
        "\n",
        "toolbox = load_tools(tool_names=[], llm=llm)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yMwqKF61HyBk"
      },
      "source": [
        "These custom tools can look into your IP address, find out where you are currently, then we will also use a secondary function to get the current date and time, then we will use this information to feed into the SerpAPI to find us the weather pattern in your area and at the time of the function calling."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "nDdVlLaAHyBk"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from datetime import datetime\n",
        "\n",
        "@tool\n",
        "def get_location_from_ip():\n",
        "    \"\"\"Get the geographical location based on the IP address.\"\"\"\n",
        "    try:\n",
        "        response = requests.get(\"https://ipinfo.io/json\")\n",
        "        data = response.json()\n",
        "        if 'loc' in data:\n",
        "            latitude, longitude = data['loc'].split(',')\n",
        "            data = (\n",
        "                f\"Latitude: {latitude},\\n\"\n",
        "                f\"Longitude: {longitude},\\n\"\n",
        "                f\"City: {data.get('city', 'N/A')},\\n\"\n",
        "                f\"Country: {data.get('country', 'N/A')}\"\n",
        "            )\n",
        "            return data\n",
        "        else:\n",
        "            return \"Location could not be determined.\"\n",
        "    except Exception as e:\n",
        "        return f\"Error occurred: {e}\"\n",
        "\n",
        "@tool\n",
        "def get_current_datetime() -> str:\n",
        "    \"\"\"Return the current date and time.\"\"\"\n",
        "    return datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P2Y9jMlLHyBl"
      },
      "source": [
        "We can create our prompt, this time we'll skip the `chat_history` part as we don't need it. However, you can add it if preferred."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "LclOCUqxHyBl"
      },
      "outputs": [],
      "source": [
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"you're a helpful assistant\"),\n",
        "    (\"human\", \"{input}\"),\n",
        "    (\"placeholder\", \"{agent_scratchpad}\")\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gp__t8sSHyBl"
      },
      "source": [
        "Now we create our full `tools` list, our `agent`, and the `agent_executor`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "a7xGa_U8HyBl"
      },
      "outputs": [],
      "source": [
        "tools = toolbox + [get_current_datetime, get_location_from_ip]\n",
        "\n",
        "agent = create_tool_calling_agent(\n",
        "    llm=llm, tools=tools, prompt=prompt\n",
        ")\n",
        "\n",
        "agent_executor = AgentExecutor(\n",
        "    agent=agent, tools=tools, verbose=True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rXIYT36aHyBl"
      },
      "source": [
        "For me I have to specify to the AI as I live in the UK and alot of places in the UK also exist in USA, which is why I explicitly state to use the country in the search as well as the town / city."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DBAxuVhVHyBl",
        "outputId": "23f936d4-fcd8-4b85-d197-998e394e9389"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_core.callbacks.manager:Error in StdOutCallbackHandler.on_chain_start callback: AttributeError(\"'NoneType' object has no attribute 'get'\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32;1m\u001b[1;3m\n",
            "Invoking: `get_current_datetime` with `{}`\n",
            "\n",
            "\n",
            "\u001b[0m\u001b[36;1m\u001b[1;3m2025-05-28 11:40:51\u001b[0m\u001b[32;1m\u001b[1;3m\n",
            "Invoking: `get_location_from_ip` with `{}`\n",
            "responded: Unfortunately, I can't access your location to provide the weather information. However, I can suggest a way to get the current weather in your location. We can use the 'get_location_from_ip' function to get your location and then use that information to get the current weather.\n",
            "\n",
            "Here's how you can do it:\n",
            "\n",
            "\n",
            "\n",
            "\u001b[0m\u001b[33;1m\u001b[1;3mLatitude: 41.2619,\n",
            "Longitude: -95.8608,\n",
            "City: Council Bluffs,\n",
            "Country: US\u001b[0m\u001b[32;1m\u001b[1;3mUnfortunately, I cannot find any information about the current weather in Council Bluffs, US.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "out = agent_executor.invoke({\n",
        "    \"input\": (\n",
        "        \"I have a few questions, what is the date and time right now? \"\n",
        "        \"How is the weather where I am? Please give me degrees in Celsius\"\n",
        "    )\n",
        "})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "id": "DNm-X4ybHyBl",
        "outputId": "10114e65-6f91-4923-9485-d20bcdc4f623"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Unfortunately, I cannot find any information about the current weather in Council Bluffs, US."
          },
          "metadata": {}
        }
      ],
      "source": [
        "from IPython.display import display, Markdown\n",
        "\n",
        "display(Markdown(out[\"output\"]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6EYV_5l3HyBl"
      },
      "source": [
        "That's the correct answer, and we even get the approximate answer in Celsius despite the tool returning the temperature in Fahrenheit.\n",
        "\n",
        "We've finished our into to LangChain Agents, in the next chapter we will be looking at how to create custom agents and executors.\n",
        "\n",
        "---"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}